{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os.path\n",
    "import random\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import DataProcessing as DP\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from functools import reduce\n",
    "import ML as ML\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import six\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, BatchNormalization, Flatten, Dense,Dropout, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from  DataProcessing import realized_volatility, trainValidSplitNp, CutAndLinear, bookDataPreprocess, combineForTraining, npToDataset\n",
    "from DataProcessing import RawDataInitialization, NpRepeatByCondition, NpSplitAndNormalized\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from ML import *\n",
    "import ML as ML\n",
    "from  DataProcessing import *\n",
    "import DataProcessing as DP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, BatchNormalization, Flatten, Dense,Dropout, Dropout, Input, Concatenate, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import time\n",
    "import math \n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearResults = pd.read_csv('LinearRegressionResults.csv')\n",
    "\n",
    "train_book, train_file, train_trade = DP.RawDataInitialization(range(0,130))\n",
    "list_of_stocks = train_book.keys()\n",
    "         \n",
    "data_frames = [pd.read_csv('globalData/'+stock_id+'.csv') for stock_id in list_of_stocks]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['time_id', 'split_60s'],\n",
    "                                             how='outer'), data_frames)\n",
    "df_merged_reindexed = df_merged.set_index(['time_id', 'split_60s']).fillna(0)\n",
    "\n",
    "df = pd.read_csv('4_inputs_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now starts stock_id=0\n"
     ]
    }
   ],
   "source": [
    "list_of_stocks_skip = df.index\n",
    "mse_4inputs = {}\n",
    "# for stock_id in list(train_book.keys()):\n",
    "stock_id='stock_id=0'\n",
    "# if stock_id  in list_of_stocks_skip:\n",
    "#     continue\n",
    "print(\"Now starts {stock_id}\".format(stock_id = stock_id))\n",
    "# stock_id = 'stock_id=52'\n",
    "stock_id_as_int = stock_id.split('=')[1]\n",
    "data, labels = DP.RawDataToBookAndTradeNp(stock_id_as_int)\n",
    "train_data, train_labels, valid_data, valid_labels = DP.trainValidSplitNp(data, labels, 0.85)\n",
    "mean = train_labels.mean()\n",
    "std = train_labels.std()\n",
    "train_labelsNormalized, valid_labelsNormalized = (train_labels-mean)/std, (valid_labels-mean)/std   \n",
    "\n",
    "\n",
    "np_test = np.transpose(np.array(df_merged_reindexed.to_xarray().to_array()), [1,2,0]).reshape(3830, 6, 112,2)\n",
    "cut = train_data.shape[0]\n",
    "globalTrain = np_test[:cut]\n",
    "globalValid = np_test[cut:]\n",
    "if (data.shape[0] != 3830):\n",
    "    with open('MissingTimeId.txt', 'w') as f:\n",
    "        f.writelines(\"{stock} has missing time_id\".format(stock = stock_id))\n",
    "#     continue\n",
    "\n",
    "interval = 120\n",
    "df2 = DP.RawDataToPd(train_book[stock_id], train_file, stock_id)\n",
    "df2['split_60s'] = df2['seconds_in_bucket']//interval\n",
    "df3 =  pd.DataFrame(df2.groupby(['stock_id','time_id','split_60s'])['log_return'].apply(realized_volatility))\n",
    "\n",
    "interval = 30\n",
    "# df4 = DP.RawDataToPd(train_book[stock_id], train_file, stock_id)\n",
    "df4 = DP.RawDataToBookAndTradePd(60)\n",
    "df4['split_{interval}s'.format(interval = interval)] = df4['seconds_in_bucket']//interval\n",
    "df5 =  pd.DataFrame(df4.groupby(['stock_id',\n",
    "                                 'time_id',\n",
    "                                 'split_{interval}s'.format(interval = interval)])[['log_return', 'log_return_trade']].apply(realized_volatility))\n",
    "\n",
    "dataFromIntervals = np.array(df3.unstack())\n",
    "dataFromSmallIntervals =np.array(df5.unstack())\n",
    "dataFromSmallIntervals = np.transpose(dataFromSmallIntervals.reshape(3830, 2, 600//interval),[0,2,1]) #2 = features = length of ['log_return', 'log_return_trade']\n",
    "\n",
    "trainFromInterval = dataFromIntervals[:cut]\n",
    "validFromInterval = dataFromIntervals[cut:]\n",
    "\n",
    "trainFromSmallInterval = dataFromSmallIntervals[:cut]\n",
    "validFromSmallInterval = dataFromSmallIntervals[cut:]\n",
    "\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices(({\"input_600s\": train_data,\n",
    "                                                    \"input_global\": globalTrain,\n",
    "                                                    'input_intervals':trainFromInterval,\n",
    "                                                    'input_smallIntervals': trainFromSmallInterval }, \n",
    "                                                   train_labelsNormalized)).batch(ML.BATCH_SIZE, drop_remainder=True).repeat(3)\n",
    "\n",
    "validDataset = tf.data.Dataset.from_tensor_slices(({\"input_600s\": valid_data, \n",
    "                                                    \"input_global\": globalValid, \n",
    "                                                    'input_intervals': validFromInterval,\n",
    "                                                    'input_smallIntervals': validFromSmallInterval}, \n",
    "                                                   valid_labelsNormalized)).batch(ML.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer using Gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from random import choices\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "\n",
    "#simply think of it as a matrix multiplication function. In reality depth = number of features, seq_len_q = seq_len_k = seq_len_v\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    for a input of (batch_size, seq_length, features), with parameters = (heads, depth). Let d_model = heads * depth\n",
    "    first use three (features , (depth * heads)) to map it into three (batch, seq_length, d_model)\n",
    "    starting from the second layer, use (d_model, d_model) to do the same thing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    refer : https://github.com/google-research/bert/blob/bee6030e31e42a9394ac567da170a89a98d2062f/modeling.py#L264\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation = gelu),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate = 0.1):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.embedding = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(self.rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        layer.get_config() can return config of this layer, and then a new layer can be \n",
    "        initialized using Layer.from_config().\n",
    "        \"\"\"\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training, mask = None):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.dropout(x, training = training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example of Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model(input_shape, num_columns, num_labels, weights = None):\n",
    "    d_model = 5\n",
    "    num_heads = 10\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = TransformerEncoder(2, d_model*num_heads, num_heads, 64)(inp)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    out = tf.keras.layers.Dense(num_labels, activation = 'linear')(x)\n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    model.compile(optimizer = 'adam', \n",
    "                  loss = tf.keras.losses.MeanSquaredError(), metrics = [tf.keras.metrics.MeanSquaredError()])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_600s (InputLayer)         [(None, 600, 2)]     0           []                               \n",
      "__________________________________________________________________________________________________\n",
      "normalization_13 (Normalizatio  (None, 600, 2)       5           ['input_600s[0][0]']             \n",
      "n)                                                                                                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 600, 32)      224         ['normalization_13[0][0]']       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 600, 32)      0           ['conv1d_15[0][0]']              \n",
      "__________________________________________________________________________________________________\n",
      "input_smallIntervals (InputLay  [(None, 20, 2)]      0           []                               \n",
      "er)                                                                                               \n",
      "__________________________________________________________________________________________________\n",
      "input_global (InputLayer)       [(None, 6, 112, 2)]  0           []                               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D  (None, 300, 32)      0           ['dropout_39[0][0]']             \n",
      ")                                                                                                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 16)       112         ['input_smallIntervals[0][0]']   \n",
      "__________________________________________________________________________________________________\n",
      "normalization_14 (Normalizatio  (None, 6, 112, 2)    5           ['input_global[0][0]']           \n",
      "n)                                                                                                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 300, 64)      6208        ['max_pooling1d_15[0][0]']       \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D  (None, 10, 16)       0           ['conv1d_17[0][0]']              \n",
      ")                                                                                                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 6, 112, 128)  2432        ['normalization_14[0][0]']       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 300, 64)      0           ['conv1d_16[0][0]']              \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)        (None, 32)           1568        ['max_pooling1d_17[0][0]']       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 6, 112, 128)  0           ['conv2d_4[0][0]']               \n",
      "__________________________________________________________________________________________________\n",
      "input_intervals (InputLayer)    [(None, 5)]          0           []                               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D  (None, 150, 64)      0           ['dropout_40[0][0]']             \n",
      ")                                                                                                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 32)           0           ['simple_rnn_5[0][0]']           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 56, 128)   0           ['dropout_47[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "normalization_15 (Normalizatio  (None, 5)            11          ['input_intervals[0][0]']        \n",
      "n)                                                                                                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 9600)         0           ['max_pooling1d_16[0][0]']       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 32)           0           ['dropout_41[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_4 (Transfo  (None, 20, 30)       15638       ['input_smallIntervals[0][0]']   \n",
      "rmerEncoder)                                                                                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 21504)        0           ['max_pooling2d_4[0][0]']        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 5)            0           ['normalization_15[0][0]']       \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 32)           307232      ['flatten_26[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 32)           1056        ['flatten_27[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 600)          0           ['transformer_encoder_4[0][0]']  \n",
      "__________________________________________________________________________________________________\n",
      "dense_114 (Dense)               (None, 48)           1032240     ['flatten_29[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 32)           192         ['flatten_30[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 1)            33          ['dense_96[0][0]']               \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 1)            33          ['dense_98[0][0]']               \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 32)           19232       ['flatten_28[0][0]']             \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 1)            49          ['dense_114[0][0]']              \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 1)            33          ['dense_116[0][0]']              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 36)           0           ['dense_97[0][0]',               \n",
      "                                                                  'dense_99[0][0]',               \n",
      "                                                                  'dense_113[0][0]',              \n",
      "                                                                  'dense_115[0][0]',              \n",
      "                                                                  'dense_117[0][0]']              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 36)           0           ['concatenate_4[0][0]']          \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 1)            37          ['flatten_31[0][0]']             \n",
      "==================================================================================================\n",
      "Total params: 1,386,340\n",
      "Trainable params: 1,386,319\n",
      "Non-trainable params: 21\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_inputs = Input(shape=(600, 2), name='input_600s')\n",
    "x = left_inputs\n",
    "filters = 32\n",
    "dropout_rate = 0.35\n",
    "weight_decay = 0.012\n",
    "x = Normalization()(x)\n",
    "for i in range(2):\n",
    "    x = Conv1D(filters = filters,\n",
    "               kernel_size= 3,\n",
    "               padding = 'same',\n",
    "               activation = 'relu',\n",
    "               kernel_regularizer=regularizers.l1(weight_decay)\n",
    "               )(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = MaxPool1D()(x)\n",
    "    filters *= 2\n",
    "x = Flatten()(x)\n",
    "x = Dense(units = 32, \n",
    "          activation='relu'\n",
    "          )(x)\n",
    "x = Dense(1)(x)\n",
    "# =============================================================================\n",
    "# second input: 10s per interval data\n",
    "# =============================================================================\n",
    "inputs_smallIntervals = Input(shape = (20,2), name = 'input_smallIntervals')\n",
    "x2 = inputs_smallIntervals\n",
    "# x2 = Normalization()(x2)\n",
    "x2 = Conv1D(filters = 16,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l1(weight_decay))(x2)\n",
    "x2 = MaxPool1D(2)(x2)\n",
    "x2 = SimpleRNN(32, \n",
    "          return_sequences=False, \n",
    "          return_state=False)(x2)\n",
    "x2 = Dropout(dropout_rate)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(32, activation='relu')(x2)\n",
    "x2 = Dense(1)(x2)\n",
    "\n",
    "\"\"\"\n",
    "transformer layer\n",
    "\"\"\"\n",
    "d_model = 3\n",
    "num_heads = 10\n",
    "x3 = TransformerEncoder(2, d_model*num_heads, num_heads, 64)(inputs_smallIntervals)\n",
    "x3 = tf.keras.layers.Flatten()(x3)\n",
    "x3 = Dense(32, activation='relu')(x3)\n",
    "# out = tf.keras.layers.Dense(num_labels, activation = 'linear')(x3)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# third input: global data\n",
    "# =============================================================================\n",
    "right_inputs = Input(shape = (6, 112, 2), name='input_global')\n",
    "y = right_inputs\n",
    "y = Normalization()(y)\n",
    "# filters = 64\n",
    "for i in range(1):\n",
    "    y = Conv2D(filters = filters,\n",
    "               kernel_size=3,\n",
    "               padding='same',\n",
    "               activation='relu',\n",
    "               kernel_regularizer=regularizers.l1(weight_decay)\n",
    "               )(y)\n",
    "    y = Dropout(dropout_rate)(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "    filters *= 2\n",
    "y = Flatten()(y)\n",
    "y = Dense(units = 48,\n",
    "          activation='relu',\n",
    "          )(y)\n",
    "y = Dense(1)(y)\n",
    "# =============================================================================\n",
    "# 4th input: 120s per interval data\n",
    "# =============================================================================\n",
    "inputs_intervals = Input(shape = (5,), name = 'input_intervals')\n",
    "z = inputs_intervals\n",
    "z = Normalization()(z)\n",
    "z = Flatten()(z)\n",
    "z = Dense(32,activation='relu')(z)\n",
    "z = Dense(1)(z)\n",
    "\n",
    "w = Concatenate()([x, x2,x3, y , z])\n",
    "# w = BatchNormalization()(w)\n",
    "w = Flatten()(w)\n",
    "# w = Dense(32,\n",
    "#           activation='relu',\n",
    "#           kernel_regularizer=regularizers.l1(weight_decay))(w)\n",
    "# w = Concatenate()([w,z])\n",
    "# w = Dense(8, activation='relu')(w)\n",
    "outputs = Dense(1)(w)\n",
    "\n",
    "model = Model([left_inputs, right_inputs, inputs_intervals, inputs_smallIntervals], outputs)\n",
    "model.compile(loss =  'mse',\n",
    "              optimizer= 'adam',\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "18/18 [==============================] - 7s 69ms/step - loss: 7.3981 - mean_squared_error: 1.4556 - val_loss: 5.9349 - val_mean_squared_error: 0.8740\n",
      "Epoch 2/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 5.4433 - mean_squared_error: 1.0801 - val_loss: 4.4718 - val_mean_squared_error: 0.8404\n",
      "Epoch 3/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.1223 - mean_squared_error: 1.0591 - val_loss: 3.3045 - val_mean_squared_error: 0.8253\n",
      "Epoch 4/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 3.1011 - mean_squared_error: 1.0526 - val_loss: 2.4422 - val_mean_squared_error: 0.8236\n",
      "Epoch 5/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 2.3546 - mean_squared_error: 1.0517 - val_loss: 1.8020 - val_mean_squared_error: 0.8172\n",
      "Epoch 6/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 1.7898 - mean_squared_error: 1.0340 - val_loss: 1.3441 - val_mean_squared_error: 0.8091\n",
      "Epoch 7/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 1.4174 - mean_squared_error: 1.0225 - val_loss: 1.0643 - val_mean_squared_error: 0.7961\n",
      "Epoch 8/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 1.2017 - mean_squared_error: 0.9987 - val_loss: 0.9248 - val_mean_squared_error: 0.7648\n",
      "Epoch 9/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 1.0895 - mean_squared_error: 0.9472 - val_loss: 0.8140 - val_mean_squared_error: 0.6883\n",
      "Epoch 10/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.9567 - mean_squared_error: 0.8412 - val_loss: 0.6794 - val_mean_squared_error: 0.5729\n",
      "Epoch 11/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.7614 - mean_squared_error: 0.6610 - val_loss: 0.5278 - val_mean_squared_error: 0.4333\n",
      "Epoch 12/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.5732 - mean_squared_error: 0.4833 - val_loss: 0.5011 - val_mean_squared_error: 0.4166\n",
      "Epoch 13/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.5224 - mean_squared_error: 0.4429 - val_loss: 0.5658 - val_mean_squared_error: 0.4914\n",
      "Epoch 14/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.4881 - mean_squared_error: 0.4160 - val_loss: 0.4548 - val_mean_squared_error: 0.3851\n",
      "Epoch 15/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.4463 - mean_squared_error: 0.3787 - val_loss: 0.4113 - val_mean_squared_error: 0.3457\n",
      "Epoch 16/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.4373 - mean_squared_error: 0.3731 - val_loss: 0.3948 - val_mean_squared_error: 0.3317\n",
      "Epoch 17/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.4267 - mean_squared_error: 0.3642 - val_loss: 0.3501 - val_mean_squared_error: 0.2877\n",
      "Epoch 18/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.4519 - mean_squared_error: 0.3894 - val_loss: 0.4087 - val_mean_squared_error: 0.3454\n",
      "Epoch 19/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.4283 - mean_squared_error: 0.3641 - val_loss: 0.4661 - val_mean_squared_error: 0.4004\n",
      "Epoch 20/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3905 - mean_squared_error: 0.3242 - val_loss: 0.4863 - val_mean_squared_error: 0.4196\n",
      "Epoch 21/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3799 - mean_squared_error: 0.3130 - val_loss: 0.4622 - val_mean_squared_error: 0.3953\n",
      "Epoch 22/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3758 - mean_squared_error: 0.3093 - val_loss: 0.4731 - val_mean_squared_error: 0.4067\n",
      "Epoch 23/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3728 - mean_squared_error: 0.3069 - val_loss: 0.4587 - val_mean_squared_error: 0.3932\n",
      "Epoch 24/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3698 - mean_squared_error: 0.3046 - val_loss: 0.4651 - val_mean_squared_error: 0.4003\n",
      "Epoch 25/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3650 - mean_squared_error: 0.3006 - val_loss: 0.4357 - val_mean_squared_error: 0.3717\n",
      "Epoch 26/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3661 - mean_squared_error: 0.3025 - val_loss: 0.4501 - val_mean_squared_error: 0.3870\n",
      "Epoch 27/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3603 - mean_squared_error: 0.2975 - val_loss: 0.4314 - val_mean_squared_error: 0.3689\n",
      "Epoch 28/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3583 - mean_squared_error: 0.2963 - val_loss: 0.4453 - val_mean_squared_error: 0.3839\n",
      "Epoch 29/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3558 - mean_squared_error: 0.2947 - val_loss: 0.4250 - val_mean_squared_error: 0.3641\n",
      "Epoch 30/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3522 - mean_squared_error: 0.2919 - val_loss: 0.4293 - val_mean_squared_error: 0.3694\n",
      "Epoch 31/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3512 - mean_squared_error: 0.2917 - val_loss: 0.4142 - val_mean_squared_error: 0.3550\n",
      "Epoch 32/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3489 - mean_squared_error: 0.2900 - val_loss: 0.4197 - val_mean_squared_error: 0.3613\n",
      "Epoch 33/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3438 - mean_squared_error: 0.2858 - val_loss: 0.4116 - val_mean_squared_error: 0.3539\n",
      "Epoch 34/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3450 - mean_squared_error: 0.2876 - val_loss: 0.4037 - val_mean_squared_error: 0.3465\n",
      "Epoch 35/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3403 - mean_squared_error: 0.2836 - val_loss: 0.3920 - val_mean_squared_error: 0.3355\n",
      "Epoch 36/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3398 - mean_squared_error: 0.2837 - val_loss: 0.3916 - val_mean_squared_error: 0.3357\n",
      "Epoch 37/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.3358 - mean_squared_error: 0.2803 - val_loss: 0.3898 - val_mean_squared_error: 0.3347\n",
      "Epoch 38/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3335 - mean_squared_error: 0.2786 - val_loss: 0.3787 - val_mean_squared_error: 0.3241\n",
      "Epoch 39/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3325 - mean_squared_error: 0.2782 - val_loss: 0.3885 - val_mean_squared_error: 0.3344\n",
      "Epoch 40/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3287 - mean_squared_error: 0.2749 - val_loss: 0.3825 - val_mean_squared_error: 0.3291\n",
      "Epoch 41/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3281 - mean_squared_error: 0.2748 - val_loss: 0.3768 - val_mean_squared_error: 0.3235\n",
      "Epoch 42/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3246 - mean_squared_error: 0.2718 - val_loss: 0.3759 - val_mean_squared_error: 0.3233\n",
      "Epoch 43/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3248 - mean_squared_error: 0.2724 - val_loss: 0.3731 - val_mean_squared_error: 0.3208\n",
      "Epoch 44/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3219 - mean_squared_error: 0.2698 - val_loss: 0.3610 - val_mean_squared_error: 0.3090\n",
      "Epoch 45/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3222 - mean_squared_error: 0.2705 - val_loss: 0.3646 - val_mean_squared_error: 0.3130\n",
      "Epoch 46/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3205 - mean_squared_error: 0.2691 - val_loss: 0.3612 - val_mean_squared_error: 0.3100\n",
      "Epoch 47/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3169 - mean_squared_error: 0.2659 - val_loss: 0.3570 - val_mean_squared_error: 0.3061\n",
      "Epoch 48/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3156 - mean_squared_error: 0.2649 - val_loss: 0.3558 - val_mean_squared_error: 0.3054\n",
      "Epoch 49/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3126 - mean_squared_error: 0.2622 - val_loss: 0.3483 - val_mean_squared_error: 0.2979\n",
      "Epoch 50/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3120 - mean_squared_error: 0.2620 - val_loss: 0.3468 - val_mean_squared_error: 0.2968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3111 - mean_squared_error: 0.2613 - val_loss: 0.3458 - val_mean_squared_error: 0.2961\n",
      "Epoch 52/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3102 - mean_squared_error: 0.2608 - val_loss: 0.3443 - val_mean_squared_error: 0.2952\n",
      "Epoch 53/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3075 - mean_squared_error: 0.2585 - val_loss: 0.3404 - val_mean_squared_error: 0.2914\n",
      "Epoch 54/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3059 - mean_squared_error: 0.2571 - val_loss: 0.3414 - val_mean_squared_error: 0.2927\n",
      "Epoch 55/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.3035 - mean_squared_error: 0.2550 - val_loss: 0.3336 - val_mean_squared_error: 0.2852\n",
      "Epoch 56/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.3017 - mean_squared_error: 0.2535 - val_loss: 0.3300 - val_mean_squared_error: 0.2818\n",
      "Epoch 57/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.3019 - mean_squared_error: 0.2538 - val_loss: 0.3332 - val_mean_squared_error: 0.2853\n",
      "Epoch 58/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2973 - mean_squared_error: 0.2495 - val_loss: 0.3253 - val_mean_squared_error: 0.2776\n",
      "Epoch 59/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2985 - mean_squared_error: 0.2509 - val_loss: 0.3255 - val_mean_squared_error: 0.2779\n",
      "Epoch 60/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2959 - mean_squared_error: 0.2485 - val_loss: 0.3212 - val_mean_squared_error: 0.2740\n",
      "Epoch 61/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2950 - mean_squared_error: 0.2478 - val_loss: 0.3195 - val_mean_squared_error: 0.2723\n",
      "Epoch 62/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2915 - mean_squared_error: 0.2446 - val_loss: 0.3200 - val_mean_squared_error: 0.2732\n",
      "Epoch 63/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2910 - mean_squared_error: 0.2443 - val_loss: 0.3149 - val_mean_squared_error: 0.2681\n",
      "Epoch 64/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2907 - mean_squared_error: 0.2441 - val_loss: 0.3180 - val_mean_squared_error: 0.2713\n",
      "Epoch 65/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2881 - mean_squared_error: 0.2417 - val_loss: 0.3090 - val_mean_squared_error: 0.2627\n",
      "Epoch 66/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2884 - mean_squared_error: 0.2421 - val_loss: 0.3092 - val_mean_squared_error: 0.2629\n",
      "Epoch 67/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2844 - mean_squared_error: 0.2384 - val_loss: 0.3069 - val_mean_squared_error: 0.2609\n",
      "Epoch 68/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2846 - mean_squared_error: 0.2386 - val_loss: 0.3081 - val_mean_squared_error: 0.2624\n",
      "Epoch 69/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2811 - mean_squared_error: 0.2354 - val_loss: 0.3036 - val_mean_squared_error: 0.2578\n",
      "Epoch 70/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2803 - mean_squared_error: 0.2347 - val_loss: 0.3043 - val_mean_squared_error: 0.2587\n",
      "Epoch 71/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2789 - mean_squared_error: 0.2334 - val_loss: 0.3026 - val_mean_squared_error: 0.2571\n",
      "Epoch 72/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2762 - mean_squared_error: 0.2309 - val_loss: 0.2992 - val_mean_squared_error: 0.2540\n",
      "Epoch 73/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2762 - mean_squared_error: 0.2310 - val_loss: 0.2986 - val_mean_squared_error: 0.2534\n",
      "Epoch 74/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2751 - mean_squared_error: 0.2300 - val_loss: 0.2966 - val_mean_squared_error: 0.2516\n",
      "Epoch 75/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2732 - mean_squared_error: 0.2283 - val_loss: 0.2934 - val_mean_squared_error: 0.2484\n",
      "Epoch 76/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2719 - mean_squared_error: 0.2271 - val_loss: 0.2901 - val_mean_squared_error: 0.2454\n",
      "Epoch 77/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2693 - mean_squared_error: 0.2247 - val_loss: 0.2905 - val_mean_squared_error: 0.2458\n",
      "Epoch 78/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2688 - mean_squared_error: 0.2242 - val_loss: 0.2860 - val_mean_squared_error: 0.2414\n",
      "Epoch 79/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2684 - mean_squared_error: 0.2239 - val_loss: 0.2842 - val_mean_squared_error: 0.2398\n",
      "Epoch 80/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2666 - mean_squared_error: 0.2223 - val_loss: 0.2848 - val_mean_squared_error: 0.2406\n",
      "Epoch 81/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2649 - mean_squared_error: 0.2206 - val_loss: 0.2828 - val_mean_squared_error: 0.2386\n",
      "Epoch 82/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2632 - mean_squared_error: 0.2191 - val_loss: 0.2768 - val_mean_squared_error: 0.2326\n",
      "Epoch 83/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2620 - mean_squared_error: 0.2179 - val_loss: 0.2785 - val_mean_squared_error: 0.2344\n",
      "Epoch 84/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2611 - mean_squared_error: 0.2172 - val_loss: 0.2726 - val_mean_squared_error: 0.2287\n",
      "Epoch 85/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2595 - mean_squared_error: 0.2157 - val_loss: 0.2735 - val_mean_squared_error: 0.2296\n",
      "Epoch 86/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2600 - mean_squared_error: 0.2162 - val_loss: 0.2728 - val_mean_squared_error: 0.2290\n",
      "Epoch 87/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2576 - mean_squared_error: 0.2140 - val_loss: 0.2685 - val_mean_squared_error: 0.2248\n",
      "Epoch 88/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2562 - mean_squared_error: 0.2126 - val_loss: 0.2690 - val_mean_squared_error: 0.2255\n",
      "Epoch 89/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2556 - mean_squared_error: 0.2121 - val_loss: 0.2655 - val_mean_squared_error: 0.2219\n",
      "Epoch 90/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2533 - mean_squared_error: 0.2099 - val_loss: 0.2632 - val_mean_squared_error: 0.2198\n",
      "Epoch 91/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2521 - mean_squared_error: 0.2087 - val_loss: 0.2613 - val_mean_squared_error: 0.2180\n",
      "Epoch 92/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2506 - mean_squared_error: 0.2074 - val_loss: 0.2597 - val_mean_squared_error: 0.2164\n",
      "Epoch 93/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2509 - mean_squared_error: 0.2076 - val_loss: 0.2586 - val_mean_squared_error: 0.2154\n",
      "Epoch 94/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2497 - mean_squared_error: 0.2066 - val_loss: 0.2588 - val_mean_squared_error: 0.2157\n",
      "Epoch 95/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2486 - mean_squared_error: 0.2057 - val_loss: 0.2550 - val_mean_squared_error: 0.2120\n",
      "Epoch 96/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2485 - mean_squared_error: 0.2055 - val_loss: 0.2522 - val_mean_squared_error: 0.2094\n",
      "Epoch 97/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2471 - mean_squared_error: 0.2043 - val_loss: 0.2523 - val_mean_squared_error: 0.2095\n",
      "Epoch 98/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2454 - mean_squared_error: 0.2027 - val_loss: 0.2506 - val_mean_squared_error: 0.2079\n",
      "Epoch 99/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2444 - mean_squared_error: 0.2018 - val_loss: 0.2474 - val_mean_squared_error: 0.2047\n",
      "Epoch 100/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2443 - mean_squared_error: 0.2017 - val_loss: 0.2480 - val_mean_squared_error: 0.2054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2446 - mean_squared_error: 0.2021 - val_loss: 0.2442 - val_mean_squared_error: 0.2017\n",
      "Epoch 102/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2424 - mean_squared_error: 0.2000 - val_loss: 0.2422 - val_mean_squared_error: 0.1999\n",
      "Epoch 103/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2419 - mean_squared_error: 0.1996 - val_loss: 0.2461 - val_mean_squared_error: 0.2037\n",
      "Epoch 104/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2393 - mean_squared_error: 0.1971 - val_loss: 0.2425 - val_mean_squared_error: 0.2002\n",
      "Epoch 105/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2411 - mean_squared_error: 0.1989 - val_loss: 0.2462 - val_mean_squared_error: 0.2039\n",
      "Epoch 106/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2386 - mean_squared_error: 0.1965 - val_loss: 0.2364 - val_mean_squared_error: 0.1943\n",
      "Epoch 107/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2381 - mean_squared_error: 0.1961 - val_loss: 0.2387 - val_mean_squared_error: 0.1967\n",
      "Epoch 108/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2390 - mean_squared_error: 0.1970 - val_loss: 0.2375 - val_mean_squared_error: 0.1956\n",
      "Epoch 109/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2373 - mean_squared_error: 0.1955 - val_loss: 0.2374 - val_mean_squared_error: 0.1955\n",
      "Epoch 110/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2351 - mean_squared_error: 0.1933 - val_loss: 0.2341 - val_mean_squared_error: 0.1923\n",
      "Epoch 111/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2383 - mean_squared_error: 0.1967 - val_loss: 0.2332 - val_mean_squared_error: 0.1916\n",
      "Epoch 112/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2348 - mean_squared_error: 0.1933 - val_loss: 0.2308 - val_mean_squared_error: 0.1893\n",
      "Epoch 113/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2336 - mean_squared_error: 0.1921 - val_loss: 0.2335 - val_mean_squared_error: 0.1920\n",
      "Epoch 114/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2333 - mean_squared_error: 0.1919 - val_loss: 0.2293 - val_mean_squared_error: 0.1879\n",
      "Epoch 115/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2335 - mean_squared_error: 0.1923 - val_loss: 0.2292 - val_mean_squared_error: 0.1879\n",
      "Epoch 116/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2328 - mean_squared_error: 0.1916 - val_loss: 0.2272 - val_mean_squared_error: 0.1863\n",
      "Epoch 117/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2292 - mean_squared_error: 0.1883 - val_loss: 0.2230 - val_mean_squared_error: 0.1820\n",
      "Epoch 118/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2295 - mean_squared_error: 0.1886 - val_loss: 0.2232 - val_mean_squared_error: 0.1825\n",
      "Epoch 119/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2298 - mean_squared_error: 0.1890 - val_loss: 0.2264 - val_mean_squared_error: 0.1855\n",
      "Epoch 120/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2301 - mean_squared_error: 0.1894 - val_loss: 0.2240 - val_mean_squared_error: 0.1832\n",
      "Epoch 121/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2299 - mean_squared_error: 0.1893 - val_loss: 0.2218 - val_mean_squared_error: 0.1813\n",
      "Epoch 122/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2282 - mean_squared_error: 0.1878 - val_loss: 0.2211 - val_mean_squared_error: 0.1806\n",
      "Epoch 123/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2282 - mean_squared_error: 0.1879 - val_loss: 0.2221 - val_mean_squared_error: 0.1819\n",
      "Epoch 124/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2270 - mean_squared_error: 0.1869 - val_loss: 0.2216 - val_mean_squared_error: 0.1815\n",
      "Epoch 125/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2268 - mean_squared_error: 0.1867 - val_loss: 0.2238 - val_mean_squared_error: 0.1838\n",
      "Epoch 126/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2263 - mean_squared_error: 0.1864 - val_loss: 0.2215 - val_mean_squared_error: 0.1816\n",
      "Epoch 127/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2251 - mean_squared_error: 0.1853 - val_loss: 0.2227 - val_mean_squared_error: 0.1830\n",
      "Epoch 128/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2256 - mean_squared_error: 0.1859 - val_loss: 0.2203 - val_mean_squared_error: 0.1808\n",
      "Epoch 129/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2251 - mean_squared_error: 0.1856 - val_loss: 0.2199 - val_mean_squared_error: 0.1803\n",
      "Epoch 130/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2250 - mean_squared_error: 0.1856 - val_loss: 0.2201 - val_mean_squared_error: 0.1808\n",
      "Epoch 131/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2246 - mean_squared_error: 0.1854 - val_loss: 0.2219 - val_mean_squared_error: 0.1826\n",
      "Epoch 132/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2242 - mean_squared_error: 0.1851 - val_loss: 0.2186 - val_mean_squared_error: 0.1796\n",
      "Epoch 133/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2232 - mean_squared_error: 0.1843 - val_loss: 0.2202 - val_mean_squared_error: 0.1814\n",
      "Epoch 134/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2238 - mean_squared_error: 0.1850 - val_loss: 0.2200 - val_mean_squared_error: 0.1809\n",
      "Epoch 135/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2215 - mean_squared_error: 0.1828 - val_loss: 0.2183 - val_mean_squared_error: 0.1796\n",
      "Epoch 136/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2217 - mean_squared_error: 0.1831 - val_loss: 0.2172 - val_mean_squared_error: 0.1787\n",
      "Epoch 137/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2229 - mean_squared_error: 0.1844 - val_loss: 0.2182 - val_mean_squared_error: 0.1797\n",
      "Epoch 138/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2209 - mean_squared_error: 0.1825 - val_loss: 0.2174 - val_mean_squared_error: 0.1791\n",
      "Epoch 139/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2218 - mean_squared_error: 0.1835 - val_loss: 0.2181 - val_mean_squared_error: 0.1798\n",
      "Epoch 140/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2210 - mean_squared_error: 0.1829 - val_loss: 0.2153 - val_mean_squared_error: 0.1774\n",
      "Epoch 141/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2228 - mean_squared_error: 0.1848 - val_loss: 0.2193 - val_mean_squared_error: 0.1813\n",
      "Epoch 142/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2212 - mean_squared_error: 0.1832 - val_loss: 0.2154 - val_mean_squared_error: 0.1774\n",
      "Epoch 143/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2222 - mean_squared_error: 0.1844 - val_loss: 0.2157 - val_mean_squared_error: 0.1779\n",
      "Epoch 144/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2200 - mean_squared_error: 0.1823 - val_loss: 0.2153 - val_mean_squared_error: 0.1775\n",
      "Epoch 145/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2203 - mean_squared_error: 0.1827 - val_loss: 0.2160 - val_mean_squared_error: 0.1785\n",
      "Epoch 146/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2206 - mean_squared_error: 0.1832 - val_loss: 0.2157 - val_mean_squared_error: 0.1784\n",
      "Epoch 147/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2208 - mean_squared_error: 0.1835 - val_loss: 0.2149 - val_mean_squared_error: 0.1775\n",
      "Epoch 148/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2204 - mean_squared_error: 0.1832 - val_loss: 0.2142 - val_mean_squared_error: 0.1769\n",
      "Epoch 149/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2192 - mean_squared_error: 0.1821 - val_loss: 0.2144 - val_mean_squared_error: 0.1773\n",
      "Epoch 150/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2198 - mean_squared_error: 0.1828 - val_loss: 0.2143 - val_mean_squared_error: 0.1775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2196 - mean_squared_error: 0.1828 - val_loss: 0.2142 - val_mean_squared_error: 0.1773\n",
      "Epoch 152/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2195 - mean_squared_error: 0.1828 - val_loss: 0.2130 - val_mean_squared_error: 0.1764\n",
      "Epoch 153/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2193 - mean_squared_error: 0.1827 - val_loss: 0.2130 - val_mean_squared_error: 0.1764\n",
      "Epoch 154/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2193 - mean_squared_error: 0.1829 - val_loss: 0.2131 - val_mean_squared_error: 0.1765\n",
      "Epoch 155/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2172 - mean_squared_error: 0.1808 - val_loss: 0.2136 - val_mean_squared_error: 0.1772\n",
      "Epoch 156/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2202 - mean_squared_error: 0.1840 - val_loss: 0.2122 - val_mean_squared_error: 0.1759\n",
      "Epoch 157/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2194 - mean_squared_error: 0.1832 - val_loss: 0.2121 - val_mean_squared_error: 0.1760\n",
      "Epoch 158/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2175 - mean_squared_error: 0.1814 - val_loss: 0.2116 - val_mean_squared_error: 0.1756\n",
      "Epoch 159/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2195 - mean_squared_error: 0.1836 - val_loss: 0.2118 - val_mean_squared_error: 0.1757\n",
      "Epoch 160/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2200 - mean_squared_error: 0.1842 - val_loss: 0.2117 - val_mean_squared_error: 0.1760\n",
      "Epoch 161/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2181 - mean_squared_error: 0.1823 - val_loss: 0.2120 - val_mean_squared_error: 0.1762\n",
      "Epoch 162/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2188 - mean_squared_error: 0.1832 - val_loss: 0.2107 - val_mean_squared_error: 0.1749\n",
      "Epoch 163/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2191 - mean_squared_error: 0.1836 - val_loss: 0.2115 - val_mean_squared_error: 0.1760\n",
      "Epoch 164/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2168 - mean_squared_error: 0.1814 - val_loss: 0.2097 - val_mean_squared_error: 0.1742\n",
      "Epoch 165/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2172 - mean_squared_error: 0.1819 - val_loss: 0.2095 - val_mean_squared_error: 0.1741\n",
      "Epoch 166/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2185 - mean_squared_error: 0.1832 - val_loss: 0.2092 - val_mean_squared_error: 0.1740\n",
      "Epoch 167/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2167 - mean_squared_error: 0.1815 - val_loss: 0.2092 - val_mean_squared_error: 0.1740\n",
      "Epoch 168/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2169 - mean_squared_error: 0.1818 - val_loss: 0.2084 - val_mean_squared_error: 0.1734\n",
      "Epoch 169/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2188 - mean_squared_error: 0.1838 - val_loss: 0.2090 - val_mean_squared_error: 0.1739\n",
      "Epoch 170/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2165 - mean_squared_error: 0.1816 - val_loss: 0.2071 - val_mean_squared_error: 0.1721\n",
      "Epoch 171/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2162 - mean_squared_error: 0.1814 - val_loss: 0.2085 - val_mean_squared_error: 0.1736\n",
      "Epoch 172/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2155 - mean_squared_error: 0.1808 - val_loss: 0.2070 - val_mean_squared_error: 0.1724\n",
      "Epoch 173/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2165 - mean_squared_error: 0.1818 - val_loss: 0.2089 - val_mean_squared_error: 0.1742\n",
      "Epoch 174/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2160 - mean_squared_error: 0.1814 - val_loss: 0.2074 - val_mean_squared_error: 0.1728\n",
      "Epoch 175/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2162 - mean_squared_error: 0.1817 - val_loss: 0.2078 - val_mean_squared_error: 0.1732\n",
      "Epoch 176/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2171 - mean_squared_error: 0.1827 - val_loss: 0.2085 - val_mean_squared_error: 0.1740\n",
      "Epoch 177/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2156 - mean_squared_error: 0.1813 - val_loss: 0.2071 - val_mean_squared_error: 0.1728\n",
      "Epoch 178/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2153 - mean_squared_error: 0.1810 - val_loss: 0.2081 - val_mean_squared_error: 0.1738\n",
      "Epoch 179/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2161 - mean_squared_error: 0.1819 - val_loss: 0.2066 - val_mean_squared_error: 0.1724\n",
      "Epoch 180/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2150 - mean_squared_error: 0.1809 - val_loss: 0.2065 - val_mean_squared_error: 0.1724\n",
      "Epoch 181/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2159 - mean_squared_error: 0.1818 - val_loss: 0.2060 - val_mean_squared_error: 0.1718\n",
      "Epoch 182/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2153 - mean_squared_error: 0.1813 - val_loss: 0.2061 - val_mean_squared_error: 0.1721\n",
      "Epoch 183/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2136 - mean_squared_error: 0.1797 - val_loss: 0.2064 - val_mean_squared_error: 0.1725\n",
      "Epoch 184/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2170 - mean_squared_error: 0.1831 - val_loss: 0.2064 - val_mean_squared_error: 0.1727\n",
      "Epoch 185/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2146 - mean_squared_error: 0.1810 - val_loss: 0.2066 - val_mean_squared_error: 0.1729\n",
      "Epoch 186/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2139 - mean_squared_error: 0.1802 - val_loss: 0.2055 - val_mean_squared_error: 0.1718\n",
      "Epoch 187/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2169 - mean_squared_error: 0.1833 - val_loss: 0.2061 - val_mean_squared_error: 0.1726\n",
      "Epoch 188/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2140 - mean_squared_error: 0.1805 - val_loss: 0.2052 - val_mean_squared_error: 0.1718\n",
      "Epoch 189/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2145 - mean_squared_error: 0.1811 - val_loss: 0.2052 - val_mean_squared_error: 0.1717\n",
      "Epoch 190/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2152 - mean_squared_error: 0.1818 - val_loss: 0.2054 - val_mean_squared_error: 0.1719\n",
      "Epoch 191/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2146 - mean_squared_error: 0.1813 - val_loss: 0.2055 - val_mean_squared_error: 0.1722\n",
      "Epoch 192/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2137 - mean_squared_error: 0.1805 - val_loss: 0.2052 - val_mean_squared_error: 0.1719\n",
      "Epoch 193/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2147 - mean_squared_error: 0.1815 - val_loss: 0.2043 - val_mean_squared_error: 0.1712\n",
      "Epoch 194/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2130 - mean_squared_error: 0.1799 - val_loss: 0.2044 - val_mean_squared_error: 0.1713\n",
      "Epoch 195/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2136 - mean_squared_error: 0.1806 - val_loss: 0.2048 - val_mean_squared_error: 0.1718\n",
      "Epoch 196/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2138 - mean_squared_error: 0.1809 - val_loss: 0.2038 - val_mean_squared_error: 0.1709\n",
      "Epoch 197/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2141 - mean_squared_error: 0.1812 - val_loss: 0.2034 - val_mean_squared_error: 0.1706\n",
      "Epoch 198/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2117 - mean_squared_error: 0.1789 - val_loss: 0.2034 - val_mean_squared_error: 0.1706\n",
      "Epoch 199/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2130 - mean_squared_error: 0.1802 - val_loss: 0.2038 - val_mean_squared_error: 0.1710\n",
      "Epoch 200/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2144 - mean_squared_error: 0.1818 - val_loss: 0.2050 - val_mean_squared_error: 0.1723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2124 - mean_squared_error: 0.1797 - val_loss: 0.2030 - val_mean_squared_error: 0.1704\n",
      "Epoch 202/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2134 - mean_squared_error: 0.1809 - val_loss: 0.2036 - val_mean_squared_error: 0.1710\n",
      "Epoch 203/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2127 - mean_squared_error: 0.1802 - val_loss: 0.2037 - val_mean_squared_error: 0.1711\n",
      "Epoch 204/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2112 - mean_squared_error: 0.1789 - val_loss: 0.2039 - val_mean_squared_error: 0.1714\n",
      "Epoch 205/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2128 - mean_squared_error: 0.1804 - val_loss: 0.2027 - val_mean_squared_error: 0.1703\n",
      "Epoch 206/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2121 - mean_squared_error: 0.1798 - val_loss: 0.2013 - val_mean_squared_error: 0.1690\n",
      "Epoch 207/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2136 - mean_squared_error: 0.1815 - val_loss: 0.2024 - val_mean_squared_error: 0.1702\n",
      "Epoch 208/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2123 - mean_squared_error: 0.1801 - val_loss: 0.2015 - val_mean_squared_error: 0.1695\n",
      "Epoch 209/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2118 - mean_squared_error: 0.1798 - val_loss: 0.2024 - val_mean_squared_error: 0.1702\n",
      "Epoch 210/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2135 - mean_squared_error: 0.1815 - val_loss: 0.2028 - val_mean_squared_error: 0.1708\n",
      "Epoch 211/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2110 - mean_squared_error: 0.1790 - val_loss: 0.2009 - val_mean_squared_error: 0.1689\n",
      "Epoch 212/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2117 - mean_squared_error: 0.1798 - val_loss: 0.2010 - val_mean_squared_error: 0.1691\n",
      "Epoch 213/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2125 - mean_squared_error: 0.1807 - val_loss: 0.2012 - val_mean_squared_error: 0.1694\n",
      "Epoch 214/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2108 - mean_squared_error: 0.1791 - val_loss: 0.2006 - val_mean_squared_error: 0.1688\n",
      "Epoch 215/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2116 - mean_squared_error: 0.1800 - val_loss: 0.2013 - val_mean_squared_error: 0.1696\n",
      "Epoch 216/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2122 - mean_squared_error: 0.1805 - val_loss: 0.2001 - val_mean_squared_error: 0.1687\n",
      "Epoch 217/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2113 - mean_squared_error: 0.1798 - val_loss: 0.2028 - val_mean_squared_error: 0.1714\n",
      "Epoch 218/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2105 - mean_squared_error: 0.1789 - val_loss: 0.1990 - val_mean_squared_error: 0.1674\n",
      "Epoch 219/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2113 - mean_squared_error: 0.1800 - val_loss: 0.2011 - val_mean_squared_error: 0.1696\n",
      "Epoch 220/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2119 - mean_squared_error: 0.1805 - val_loss: 0.2010 - val_mean_squared_error: 0.1697\n",
      "Epoch 221/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2112 - mean_squared_error: 0.1799 - val_loss: 0.2000 - val_mean_squared_error: 0.1688\n",
      "Epoch 222/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2115 - mean_squared_error: 0.1803 - val_loss: 0.2000 - val_mean_squared_error: 0.1687\n",
      "Epoch 223/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2114 - mean_squared_error: 0.1802 - val_loss: 0.2007 - val_mean_squared_error: 0.1695\n",
      "Epoch 224/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2118 - mean_squared_error: 0.1808 - val_loss: 0.1999 - val_mean_squared_error: 0.1688\n",
      "Epoch 225/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2104 - mean_squared_error: 0.1793 - val_loss: 0.1997 - val_mean_squared_error: 0.1686\n",
      "Epoch 226/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2108 - mean_squared_error: 0.1799 - val_loss: 0.1993 - val_mean_squared_error: 0.1684\n",
      "Epoch 227/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2105 - mean_squared_error: 0.1796 - val_loss: 0.1994 - val_mean_squared_error: 0.1684\n",
      "Epoch 228/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2088 - mean_squared_error: 0.1779 - val_loss: 0.1997 - val_mean_squared_error: 0.1689\n",
      "Epoch 229/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2107 - mean_squared_error: 0.1799 - val_loss: 0.2000 - val_mean_squared_error: 0.1690\n",
      "Epoch 230/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2108 - mean_squared_error: 0.1800 - val_loss: 0.1993 - val_mean_squared_error: 0.1685\n",
      "Epoch 231/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2085 - mean_squared_error: 0.1778 - val_loss: 0.1985 - val_mean_squared_error: 0.1677\n",
      "Epoch 232/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2103 - mean_squared_error: 0.1796 - val_loss: 0.2002 - val_mean_squared_error: 0.1693\n",
      "Epoch 233/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2098 - mean_squared_error: 0.1792 - val_loss: 0.2008 - val_mean_squared_error: 0.1702\n",
      "Epoch 234/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2093 - mean_squared_error: 0.1787 - val_loss: 0.2004 - val_mean_squared_error: 0.1698\n",
      "Epoch 235/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2099 - mean_squared_error: 0.1793 - val_loss: 0.1996 - val_mean_squared_error: 0.1690\n",
      "Epoch 236/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2087 - mean_squared_error: 0.1782 - val_loss: 0.1996 - val_mean_squared_error: 0.1691\n",
      "Epoch 237/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2090 - mean_squared_error: 0.1786 - val_loss: 0.1999 - val_mean_squared_error: 0.1694\n",
      "Epoch 238/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2099 - mean_squared_error: 0.1795 - val_loss: 0.2004 - val_mean_squared_error: 0.1701\n",
      "Epoch 239/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2090 - mean_squared_error: 0.1787 - val_loss: 0.2006 - val_mean_squared_error: 0.1702\n",
      "Epoch 240/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2114 - mean_squared_error: 0.1811 - val_loss: 0.2011 - val_mean_squared_error: 0.1709\n",
      "Epoch 241/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2078 - mean_squared_error: 0.1775 - val_loss: 0.1983 - val_mean_squared_error: 0.1680\n",
      "Epoch 242/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2078 - mean_squared_error: 0.1776 - val_loss: 0.1993 - val_mean_squared_error: 0.1692\n",
      "Epoch 243/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2082 - mean_squared_error: 0.1782 - val_loss: 0.1980 - val_mean_squared_error: 0.1679\n",
      "Epoch 244/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2091 - mean_squared_error: 0.1790 - val_loss: 0.1966 - val_mean_squared_error: 0.1666\n",
      "Epoch 245/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2091 - mean_squared_error: 0.1790 - val_loss: 0.2042 - val_mean_squared_error: 0.1741\n",
      "Epoch 246/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2084 - mean_squared_error: 0.1784 - val_loss: 0.2014 - val_mean_squared_error: 0.1713\n",
      "Epoch 247/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2080 - mean_squared_error: 0.1781 - val_loss: 0.1991 - val_mean_squared_error: 0.1691\n",
      "Epoch 248/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2081 - mean_squared_error: 0.1782 - val_loss: 0.2001 - val_mean_squared_error: 0.1703\n",
      "Epoch 249/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2077 - mean_squared_error: 0.1778 - val_loss: 0.2034 - val_mean_squared_error: 0.1735\n",
      "Epoch 250/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2085 - mean_squared_error: 0.1787 - val_loss: 0.2020 - val_mean_squared_error: 0.1723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2068 - mean_squared_error: 0.1771 - val_loss: 0.1995 - val_mean_squared_error: 0.1696\n",
      "Epoch 252/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2066 - mean_squared_error: 0.1769 - val_loss: 0.2025 - val_mean_squared_error: 0.1729\n",
      "Epoch 253/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2078 - mean_squared_error: 0.1781 - val_loss: 0.1985 - val_mean_squared_error: 0.1687\n",
      "Epoch 254/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2085 - mean_squared_error: 0.1789 - val_loss: 0.2012 - val_mean_squared_error: 0.1714\n",
      "Epoch 255/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2069 - mean_squared_error: 0.1773 - val_loss: 0.1954 - val_mean_squared_error: 0.1658\n",
      "Epoch 256/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2070 - mean_squared_error: 0.1774 - val_loss: 0.1969 - val_mean_squared_error: 0.1675\n",
      "Epoch 257/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2059 - mean_squared_error: 0.1765 - val_loss: 0.1980 - val_mean_squared_error: 0.1685\n",
      "Epoch 258/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2063 - mean_squared_error: 0.1769 - val_loss: 0.1991 - val_mean_squared_error: 0.1697\n",
      "Epoch 259/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2079 - mean_squared_error: 0.1784 - val_loss: 0.2009 - val_mean_squared_error: 0.1715\n",
      "Epoch 260/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2049 - mean_squared_error: 0.1756 - val_loss: 0.2041 - val_mean_squared_error: 0.1747\n",
      "Epoch 261/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2040 - mean_squared_error: 0.1747 - val_loss: 0.1954 - val_mean_squared_error: 0.1661\n",
      "Epoch 262/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2073 - mean_squared_error: 0.1781 - val_loss: 0.1968 - val_mean_squared_error: 0.1675\n",
      "Epoch 263/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2065 - mean_squared_error: 0.1774 - val_loss: 0.2039 - val_mean_squared_error: 0.1748\n",
      "Epoch 264/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2045 - mean_squared_error: 0.1753 - val_loss: 0.2029 - val_mean_squared_error: 0.1737\n",
      "Epoch 265/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2053 - mean_squared_error: 0.1763 - val_loss: 0.2038 - val_mean_squared_error: 0.1748\n",
      "Epoch 266/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2046 - mean_squared_error: 0.1756 - val_loss: 0.2097 - val_mean_squared_error: 0.1806\n",
      "Epoch 267/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2030 - mean_squared_error: 0.1739 - val_loss: 0.1985 - val_mean_squared_error: 0.1695\n",
      "Epoch 268/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2030 - mean_squared_error: 0.1741 - val_loss: 0.1995 - val_mean_squared_error: 0.1705\n",
      "Epoch 269/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2049 - mean_squared_error: 0.1760 - val_loss: 0.2064 - val_mean_squared_error: 0.1774\n",
      "Epoch 270/2000\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.2028 - mean_squared_error: 0.1739 - val_loss: 0.2085 - val_mean_squared_error: 0.1796\n",
      "Epoch 271/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2044 - mean_squared_error: 0.1755 - val_loss: 0.2054 - val_mean_squared_error: 0.1764\n",
      "Epoch 272/2000\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.2035 - mean_squared_error: 0.1746 - val_loss: 0.1941 - val_mean_squared_error: 0.1655\n",
      "Epoch 273/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2069 - mean_squared_error: 0.1781 - val_loss: 0.1993 - val_mean_squared_error: 0.1705\n",
      "Epoch 274/2000\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.2028 - mean_squared_error: 0.1740 - val_loss: 0.1952 - val_mean_squared_error: 0.1660\n",
      "Epoch 275/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2008 - mean_squared_error: 0.1721 - val_loss: 0.2007 - val_mean_squared_error: 0.1720\n",
      "Epoch 276/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2029 - mean_squared_error: 0.1742 - val_loss: 0.1998 - val_mean_squared_error: 0.1711\n",
      "Epoch 277/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2050 - mean_squared_error: 0.1764 - val_loss: 0.2006 - val_mean_squared_error: 0.1720\n",
      "Epoch 278/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2008 - mean_squared_error: 0.1722 - val_loss: 0.1971 - val_mean_squared_error: 0.1685\n",
      "Epoch 279/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2026 - mean_squared_error: 0.1740 - val_loss: 0.2022 - val_mean_squared_error: 0.1735\n",
      "Epoch 280/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2016 - mean_squared_error: 0.1731 - val_loss: 0.2017 - val_mean_squared_error: 0.1732\n",
      "Epoch 281/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2023 - mean_squared_error: 0.1738 - val_loss: 0.1939 - val_mean_squared_error: 0.1653\n",
      "Epoch 282/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2022 - mean_squared_error: 0.1737 - val_loss: 0.2014 - val_mean_squared_error: 0.1730\n",
      "Epoch 283/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1989 - mean_squared_error: 0.1705 - val_loss: 0.2020 - val_mean_squared_error: 0.1735\n",
      "Epoch 284/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1994 - mean_squared_error: 0.1710 - val_loss: 0.2039 - val_mean_squared_error: 0.1755\n",
      "Epoch 285/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1991 - mean_squared_error: 0.1708 - val_loss: 0.2006 - val_mean_squared_error: 0.1721\n",
      "Epoch 286/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2025 - mean_squared_error: 0.1741 - val_loss: 0.2027 - val_mean_squared_error: 0.1745\n",
      "Epoch 287/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2076 - mean_squared_error: 0.1793 - val_loss: 0.2004 - val_mean_squared_error: 0.1719\n",
      "Epoch 288/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.2093 - mean_squared_error: 0.1810 - val_loss: 0.1976 - val_mean_squared_error: 0.1693\n",
      "Epoch 289/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.2038 - mean_squared_error: 0.1756 - val_loss: 0.2182 - val_mean_squared_error: 0.1899\n",
      "Epoch 290/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2000 - mean_squared_error: 0.1717 - val_loss: 0.2098 - val_mean_squared_error: 0.1814\n",
      "Epoch 291/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1962 - mean_squared_error: 0.1680 - val_loss: 0.1967 - val_mean_squared_error: 0.1683\n",
      "Epoch 292/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1965 - mean_squared_error: 0.1683 - val_loss: 0.2163 - val_mean_squared_error: 0.1881\n",
      "Epoch 293/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1981 - mean_squared_error: 0.1698 - val_loss: 0.2032 - val_mean_squared_error: 0.1749\n",
      "Epoch 294/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2077 - mean_squared_error: 0.1796 - val_loss: 0.1947 - val_mean_squared_error: 0.1665\n",
      "Epoch 295/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2099 - mean_squared_error: 0.1819 - val_loss: 0.2066 - val_mean_squared_error: 0.1786\n",
      "Epoch 296/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2054 - mean_squared_error: 0.1774 - val_loss: 0.2092 - val_mean_squared_error: 0.1812\n",
      "Epoch 297/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1982 - mean_squared_error: 0.1701 - val_loss: 0.2011 - val_mean_squared_error: 0.1730\n",
      "Epoch 298/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1962 - mean_squared_error: 0.1681 - val_loss: 0.1998 - val_mean_squared_error: 0.1718\n",
      "Epoch 299/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1942 - mean_squared_error: 0.1662 - val_loss: 0.2161 - val_mean_squared_error: 0.1880\n",
      "Epoch 300/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1945 - mean_squared_error: 0.1666 - val_loss: 0.2095 - val_mean_squared_error: 0.1816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1948 - mean_squared_error: 0.1669 - val_loss: 0.2019 - val_mean_squared_error: 0.1739\n",
      "Epoch 302/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1941 - mean_squared_error: 0.1664 - val_loss: 0.1976 - val_mean_squared_error: 0.1696\n",
      "Epoch 303/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1924 - mean_squared_error: 0.1646 - val_loss: 0.1983 - val_mean_squared_error: 0.1704\n",
      "Epoch 304/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1919 - mean_squared_error: 0.1641 - val_loss: 0.2147 - val_mean_squared_error: 0.1869\n",
      "Epoch 305/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1913 - mean_squared_error: 0.1636 - val_loss: 0.2036 - val_mean_squared_error: 0.1759\n",
      "Epoch 306/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1942 - mean_squared_error: 0.1665 - val_loss: 0.2047 - val_mean_squared_error: 0.1771\n",
      "Epoch 307/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1899 - mean_squared_error: 0.1623 - val_loss: 0.1971 - val_mean_squared_error: 0.1693\n",
      "Epoch 308/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1909 - mean_squared_error: 0.1632 - val_loss: 0.1995 - val_mean_squared_error: 0.1718\n",
      "Epoch 309/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1900 - mean_squared_error: 0.1624 - val_loss: 0.2050 - val_mean_squared_error: 0.1774\n",
      "Epoch 310/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1878 - mean_squared_error: 0.1602 - val_loss: 0.2062 - val_mean_squared_error: 0.1786\n",
      "Epoch 311/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1855 - mean_squared_error: 0.1579 - val_loss: 0.2028 - val_mean_squared_error: 0.1752\n",
      "Epoch 312/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1869 - mean_squared_error: 0.1595 - val_loss: 0.2177 - val_mean_squared_error: 0.1903\n",
      "Epoch 313/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1924 - mean_squared_error: 0.1650 - val_loss: 0.2044 - val_mean_squared_error: 0.1769\n",
      "Epoch 314/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1887 - mean_squared_error: 0.1613 - val_loss: 0.2081 - val_mean_squared_error: 0.1807\n",
      "Epoch 315/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1883 - mean_squared_error: 0.1609 - val_loss: 0.2043 - val_mean_squared_error: 0.1769\n",
      "Epoch 316/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1860 - mean_squared_error: 0.1587 - val_loss: 0.2041 - val_mean_squared_error: 0.1767\n",
      "Epoch 317/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1864 - mean_squared_error: 0.1591 - val_loss: 0.1936 - val_mean_squared_error: 0.1663\n",
      "Epoch 318/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1848 - mean_squared_error: 0.1576 - val_loss: 0.2066 - val_mean_squared_error: 0.1794\n",
      "Epoch 319/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1851 - mean_squared_error: 0.1580 - val_loss: 0.2119 - val_mean_squared_error: 0.1847\n",
      "Epoch 320/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1859 - mean_squared_error: 0.1588 - val_loss: 0.2042 - val_mean_squared_error: 0.1771\n",
      "Epoch 321/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1829 - mean_squared_error: 0.1558 - val_loss: 0.2019 - val_mean_squared_error: 0.1747\n",
      "Epoch 322/2000\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.1864 - mean_squared_error: 0.1594 - val_loss: 0.1893 - val_mean_squared_error: 0.1623\n",
      "Epoch 323/2000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.1867 - mean_squared_error: 0.1597 - val_loss: 0.1903 - val_mean_squared_error: 0.1633\n",
      "Epoch 324/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1842 - mean_squared_error: 0.1573 - val_loss: 0.2094 - val_mean_squared_error: 0.1823\n",
      "Epoch 325/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1816 - mean_squared_error: 0.1546 - val_loss: 0.1960 - val_mean_squared_error: 0.1690\n",
      "Epoch 326/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1795 - mean_squared_error: 0.1526 - val_loss: 0.1921 - val_mean_squared_error: 0.1654\n",
      "Epoch 327/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1814 - mean_squared_error: 0.1546 - val_loss: 0.2046 - val_mean_squared_error: 0.1779\n",
      "Epoch 328/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1791 - mean_squared_error: 0.1523 - val_loss: 0.2009 - val_mean_squared_error: 0.1743\n",
      "Epoch 329/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1791 - mean_squared_error: 0.1524 - val_loss: 0.2014 - val_mean_squared_error: 0.1745\n",
      "Epoch 330/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1751 - mean_squared_error: 0.1484 - val_loss: 0.2041 - val_mean_squared_error: 0.1772\n",
      "Epoch 331/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1738 - mean_squared_error: 0.1472 - val_loss: 0.1885 - val_mean_squared_error: 0.1619\n",
      "Epoch 332/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1754 - mean_squared_error: 0.1488 - val_loss: 0.1967 - val_mean_squared_error: 0.1702\n",
      "Epoch 333/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1732 - mean_squared_error: 0.1466 - val_loss: 0.1983 - val_mean_squared_error: 0.1718\n",
      "Epoch 334/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1720 - mean_squared_error: 0.1454 - val_loss: 0.1948 - val_mean_squared_error: 0.1681\n",
      "Epoch 335/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1736 - mean_squared_error: 0.1471 - val_loss: 0.1918 - val_mean_squared_error: 0.1653\n",
      "Epoch 336/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1751 - mean_squared_error: 0.1487 - val_loss: 0.1888 - val_mean_squared_error: 0.1624\n",
      "Epoch 337/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1710 - mean_squared_error: 0.1445 - val_loss: 0.1999 - val_mean_squared_error: 0.1735\n",
      "Epoch 338/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1687 - mean_squared_error: 0.1423 - val_loss: 0.2066 - val_mean_squared_error: 0.1801\n",
      "Epoch 339/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1751 - mean_squared_error: 0.1488 - val_loss: 0.1988 - val_mean_squared_error: 0.1724\n",
      "Epoch 340/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1693 - mean_squared_error: 0.1429 - val_loss: 0.1995 - val_mean_squared_error: 0.1732\n",
      "Epoch 341/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1678 - mean_squared_error: 0.1416 - val_loss: 0.1915 - val_mean_squared_error: 0.1653\n",
      "Epoch 342/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1700 - mean_squared_error: 0.1438 - val_loss: 0.1969 - val_mean_squared_error: 0.1706\n",
      "Epoch 343/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1676 - mean_squared_error: 0.1415 - val_loss: 0.1970 - val_mean_squared_error: 0.1709\n",
      "Epoch 344/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1649 - mean_squared_error: 0.1388 - val_loss: 0.2000 - val_mean_squared_error: 0.1737\n",
      "Epoch 345/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1667 - mean_squared_error: 0.1405 - val_loss: 0.1977 - val_mean_squared_error: 0.1717\n",
      "Epoch 346/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1610 - mean_squared_error: 0.1349 - val_loss: 0.1980 - val_mean_squared_error: 0.1719\n",
      "Epoch 347/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1640 - mean_squared_error: 0.1380 - val_loss: 0.2089 - val_mean_squared_error: 0.1829\n",
      "Epoch 348/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1591 - mean_squared_error: 0.1330 - val_loss: 0.2174 - val_mean_squared_error: 0.1916\n",
      "Epoch 349/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1596 - mean_squared_error: 0.1337 - val_loss: 0.2009 - val_mean_squared_error: 0.1748\n",
      "Epoch 350/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1650 - mean_squared_error: 0.1391 - val_loss: 0.2125 - val_mean_squared_error: 0.1866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1577 - mean_squared_error: 0.1319 - val_loss: 0.2223 - val_mean_squared_error: 0.1965\n",
      "Epoch 352/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1580 - mean_squared_error: 0.1323 - val_loss: 0.2123 - val_mean_squared_error: 0.1865\n",
      "Epoch 353/2000\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.1591 - mean_squared_error: 0.1335 - val_loss: 0.2158 - val_mean_squared_error: 0.1901\n",
      "Epoch 354/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1540 - mean_squared_error: 0.1283 - val_loss: 0.2025 - val_mean_squared_error: 0.1768\n",
      "Epoch 355/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1530 - mean_squared_error: 0.1274 - val_loss: 0.2138 - val_mean_squared_error: 0.1882\n",
      "Epoch 356/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1534 - mean_squared_error: 0.1279 - val_loss: 0.2017 - val_mean_squared_error: 0.1763\n",
      "Epoch 357/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1526 - mean_squared_error: 0.1271 - val_loss: 0.2147 - val_mean_squared_error: 0.1891\n",
      "Epoch 358/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1469 - mean_squared_error: 0.1214 - val_loss: 0.2127 - val_mean_squared_error: 0.1872\n",
      "Epoch 359/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1519 - mean_squared_error: 0.1266 - val_loss: 0.2023 - val_mean_squared_error: 0.1767\n",
      "Epoch 360/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1606 - mean_squared_error: 0.1352 - val_loss: 0.2230 - val_mean_squared_error: 0.1977\n",
      "Epoch 361/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1491 - mean_squared_error: 0.1238 - val_loss: 0.2188 - val_mean_squared_error: 0.1936\n",
      "Epoch 362/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1456 - mean_squared_error: 0.1204 - val_loss: 0.2105 - val_mean_squared_error: 0.1852\n",
      "Epoch 363/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1464 - mean_squared_error: 0.1212 - val_loss: 0.2408 - val_mean_squared_error: 0.2155\n",
      "Epoch 364/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1447 - mean_squared_error: 0.1195 - val_loss: 0.2052 - val_mean_squared_error: 0.1800\n",
      "Epoch 365/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1406 - mean_squared_error: 0.1155 - val_loss: 0.2221 - val_mean_squared_error: 0.1969\n",
      "Epoch 366/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1399 - mean_squared_error: 0.1149 - val_loss: 0.2224 - val_mean_squared_error: 0.1974\n",
      "Epoch 367/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1512 - mean_squared_error: 0.1262 - val_loss: 0.2110 - val_mean_squared_error: 0.1860\n",
      "Epoch 368/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1405 - mean_squared_error: 0.1155 - val_loss: 0.2144 - val_mean_squared_error: 0.1894\n",
      "Epoch 369/2000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1405 - mean_squared_error: 0.1156 - val_loss: 0.2158 - val_mean_squared_error: 0.1908\n",
      "Epoch 370/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1418 - mean_squared_error: 0.1169 - val_loss: 0.2323 - val_mean_squared_error: 0.2074\n",
      "Epoch 371/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1424 - mean_squared_error: 0.1175 - val_loss: 0.2076 - val_mean_squared_error: 0.1826\n",
      "Epoch 372/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1483 - mean_squared_error: 0.1233 - val_loss: 0.2407 - val_mean_squared_error: 0.2156\n",
      "Epoch 373/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1440 - mean_squared_error: 0.1192 - val_loss: 0.2161 - val_mean_squared_error: 0.1913\n",
      "Epoch 374/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1375 - mean_squared_error: 0.1127 - val_loss: 0.2115 - val_mean_squared_error: 0.1866\n",
      "Epoch 375/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1346 - mean_squared_error: 0.1100 - val_loss: 0.2177 - val_mean_squared_error: 0.1930\n",
      "Epoch 376/2000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.1348 - mean_squared_error: 0.1101 - val_loss: 0.2187 - val_mean_squared_error: 0.1941\n",
      "Epoch 377/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1306 - mean_squared_error: 0.1060 - val_loss: 0.2134 - val_mean_squared_error: 0.1888\n",
      "Epoch 378/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1323 - mean_squared_error: 0.1078 - val_loss: 0.2025 - val_mean_squared_error: 0.1780\n",
      "Epoch 379/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1405 - mean_squared_error: 0.1160 - val_loss: 0.2372 - val_mean_squared_error: 0.2126\n",
      "Epoch 380/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1397 - mean_squared_error: 0.1152 - val_loss: 0.2061 - val_mean_squared_error: 0.1816\n",
      "Epoch 381/2000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.1440 - mean_squared_error: 0.1195 - val_loss: 0.2169 - val_mean_squared_error: 0.1922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25171d8f790>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point, early_stopping, learning_rate_reduction = ML.get_callbacks('test', \n",
    "                                                                        monitor='val_mean_squared_error', mode='min')\n",
    "model.fit(trainDataset, validation_data=validDataset, epochs=2000, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
